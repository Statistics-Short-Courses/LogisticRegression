[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Logistic Regression with R",
    "section": "",
    "text": "Course Overview",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#who-this-course-is-for",
    "href": "index.html#who-this-course-is-for",
    "title": "Logistic Regression with R",
    "section": "Who this course is for",
    "text": "Who this course is for\nIf you have completed the Linear Regression Short Course, you have the foundation to build a wide range of statistical models for data analysis. However, linear regression is not suitable for all types of data, and it is common to encounter data in which the outcome variable is binary (e.g., success/failure, yes/no). Logistic regression is a powerful tool for modeling binary outcomes, and this course will guide you through its concepts and applications.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Logistic Regression with R",
    "section": "What you will learn",
    "text": "What you will learn\n\nUnderstand the difference between linear and logistic regression.\nRecognize when logistic regression is appropriate for binary outcomes.\nTranslate between probability, odds, and log-odds.\nFit logistic models in R with glm() and read the output.\nInterpret coefficients as odds ratios and communicate effects clearly.\nAssess model performance with likelihood measures and classification metrics.\nMake predictions and evaluate classification accuracy.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Logistic Regression with R",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou have completed the Linear Regression short course (or equivalent).\nYou can run basic R scripts and read simple model summaries.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-map",
    "href": "index.html#course-map",
    "title": "Logistic Regression with R",
    "section": "Course map",
    "text": "Course map\n\nWhen to Use Logistic Regression when to use logistic regression (and when linear regression fails).\nIntroduction to Logistic Regression (GLMs, odds/logit, logistic function).\nFitting Logistic Regression Models logistic regression models (estimation, interpretation, inference).\n?sec-assessing model fit (deviance, model comparison, pseudo R-squared).\nPredictions from Logistic Regression Models from logistic regression models (probabilities, thresholds, ROC/AUC).\n?sec-extensions and mini case study (optional: multivariable models, interactions, reporting).",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Logistic Regression with R",
    "section": "How to use this book",
    "text": "How to use this book\n\nThe course is designed to be completed sequentially: successive chapters build on prior concepts. However, it is possible to jump to specific sections as needed.\nThere is a glossary at the end for quick reference of key terms.\nEach chapter includes excersises which should be attempted before continuing.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "ch_1_motivation.html",
    "href": "ch_1_motivation.html",
    "title": "1  Introduction to Logistic Regression",
    "section": "",
    "text": "1.1 Motivation: modelling binary outcomes\nIn the Linear Regression Short Course (LRSC), we used a linear model to predict a variable Y from one or more predictors X. Something like this:\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\epsilon\n\\]\nIn the cases we considered, Y was a continuous variable (e.g., salary, Naplan score, etc.), and the model predicted values of Y that could take on any real number.\nIn logistic regression, our outcome is not continuous. Instead, it is typically a discrete categorical variable. In particular, logistic regression is used to model a binary variable i.e. one which takes on only two possible values (e.g., success/failure, yes/no, 0/1)1.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Logistic Regression</span>"
    ]
  },
  {
    "objectID": "ch_1_motivation.html#sec-motivation",
    "href": "ch_1_motivation.html#sec-motivation",
    "title": "1  Introduction to Logistic Regression",
    "section": "",
    "text": "Example 1.1: A binary outcome: pass or fail\nJohno is a student studying for an important exam. He wants to understand how his study time affects his chances of passing the exam. To this end, he collects data on previous students’ study times and whether they passed or failed the exam. Here is a sample of the data Johno collected:\n\n\n# A tibble: 395 × 2\n   studytime pass_fail\n       &lt;dbl&gt; &lt;chr&gt;    \n 1         2 fail     \n 2         2 fail     \n 3         2 pass     \n 4         3 pass     \n 5         2 pass     \n 6         2 pass     \n 7         2 pass     \n 8         2 fail     \n 9         2 pass     \n10         2 pass     \n# ℹ 385 more rows\n\n\nJohno wants to use this data to model the relationship between study time and the probability of passing the exam. He identifies his outcome variable, \\(Y\\), as the binary variable indicating whether a student passed or failed the exam, and his predictor variable, \\(X\\), as the amount of time spent studying. Therefore, Johno’s goal is to produce a statistical model that predicts Y (pass/fail) based on X (study time): \\[\nY = f(X) + \\epsilon\n\\]\n\n\n\n1.1.1 From categories to discrete numbers\nAs with dummy variables for categorical predictors in linear regression, we often recode a binary outcome into numbers so we can work with it mathematically. Conventionally we use \\(Y=1\\) for the event of interest (e.g., “pass”) and \\(Y=0\\) otherwise—this coding doesn’t make the outcome “really numeric”, it’s just a convenient label that lets us model probabilities.\n\n\n1.1.2 From modelling categories to modelling probabilities\nWhen the outcome is binary (pass/fail, yes/no, 0/1), it can be tempting to treat the problem as “predict the category”. But in most real settings the category is not deterministic given the predictors: even for the same study time, two students might differ in prior knowledge, sleep, assessment difficulty, and so on. So rather than trying to claim certainty (“this student will pass”), a statistical model aims to quantify how likely the event is.\nFor a binary outcome, it is useful to code the outcome as \\[\nY =\n\\\\begin{cases}\n1 & \\\\text{event occurs (e.g., pass)} \\\\\\\\\n0 & \\\\text{event does not occur (e.g., fail)}\n\\\\end{cases}\n\\] and model the probability of the event given the predictors: \\[\np(x) = P(Y = 1 \\\\mid X = x).\n\\] This is a shift from predicting a label (two discrete options) to modelling a probability (a continuum between 0 and 1). Once we have a probability model, “predicting a category” is just a decision rule on top of it (e.g., predict “pass” if \\(p(x) \\\\ge 0.5\\)), and that cutoff can change depending on the costs of false positives/negatives.\nOne reason linear regression seems to “almost work” here is that when \\(Y\\) is coded as 0/1, \\[\nE(Y \\\\mid X = x) = P(Y = 1 \\\\mid X = x) = p(x),\n\\] so modelling the mean looks like modelling a probability. The issue is that an ordinary linear model does not respect the special nature of probabilities (they must stay between 0 and 1), which motivates logistic regression.\n\nExample 1.2: When linear regression is not appropriate\nSuppose we want to predict whether a student passes or fails a course based on their study hours and have obtained a dataset like this:\n\n\n# A tibble: 395 × 2\n   studytime pass_fail\n       &lt;dbl&gt; &lt;chr&gt;    \n 1         2 fail     \n 2         2 fail     \n 3         2 pass     \n 4         3 pass     \n 5         2 pass     \n 6         2 pass     \n 7         2 pass     \n 8         2 fail     \n 9         2 pass     \n10         2 pass     \n# ℹ 385 more rows\n\n\nLet’s follow the steps we might take if we tried to use linear regression here. First, we convert the categorical outcome into a numeric 0/1 variable (remember: the mean of a 0/1 variable is a probability). As in previous courses, we can use 1 for “pass” and 0 for “fail”:\nLet’s plot the data in a scatter plot with a fitted linear regression line:\n\n\n\n\n\n\n\n\n\n(note the use of geom_jitter() to add some random noise to the points so they don’t overlap exactly).\nHow do we interpret the fitted line? Notice that for any of the study times in our data, the predicted value from the linear regression line is between 0 (fail) and 1(pass).\n\n\n        1         2         3         4 \n0.6694035 0.7112321 0.6275749 0.7530607 \n\n\nThis makes sense since the ‘highest score’ of any student in our dataset is 1 (i.e. a pass), and the ‘lowest score’ is 0 (a fail). No student has a predicted score of exactly 0 or 1, but the predicted values are between these two extremes. We might reasonably interpret the predicted value as the probability of passing the course given the study time (i.e. the coefficient for study time indicates how much the probability of passing increases for each additional hour of study time).\nFor example, a student who studies for 2 hours has a predicted probability of passing of about 0.6694035, while a student who studies for 4 hours has a predicted probability of passing of about 0.7530607.\nHowever, the linear regression line can produce predicted values outside this range (less than 0 or greater than 1), which does not make sense for probabilities.\nFor example, a student who studies for 0 hours has a predicted probability of passing of about 0.5857463, which is less than 0, and a student who studies for 10 hours has a predicted probability of passing of about 1.0040323, which is greater than 1. With a binary outcome, the relationship between predictors and probability is typically nonlinear: the “same” change in a predictor cannot produce the same change in probability at every starting probability (e.g., you can’t increase a 0.95 probability by +0.2).\n\n\n\n\n1.1.3 Examples of binary outcomes\nBinary outcomes show up in many applied settings:\n\nDisease status (yes/no)\nAdmission (accepted/rejected)\nSurvival (alive/dead)\nPurchase (bought/did not buy)\n\n\n\n1.1.4 Why linear regression fails for binary outcomes\nWith a binary outcome, the relationship between predictors and probability is typically nonlinear: the “same” change in a predictor cannot produce the same change in probability at every starting probability (e.g., you can’t increase a 0.95 probability by +0.2).\nLogistic regression fixes this by modeling a linear relationship on the log-odds (logit) scale, then converting back to the probability scale.\n\n\n1.1.5 What logistic regression provides\nLogistic regression models the probability of an event while ensuring predictions stay between 0 and 1. It also supports effect interpretation via odds ratios and provides likelihood-based tools for inference and model assessment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Logistic Regression</span>"
    ]
  },
  {
    "objectID": "ch_1_motivation.html#examples-of-binary-outcomes",
    "href": "ch_1_motivation.html#examples-of-binary-outcomes",
    "title": "1  Introduction to Logistic Regression",
    "section": "1.2 Examples of binary outcomes",
    "text": "1.2 Examples of binary outcomes\nBinary outcomes show up in many applied settings:\n\nDisease status (yes/no)\nAdmission (accepted/rejected)\nSurvival (alive/dead)\nPurchase (bought/did not buy)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Logistic Regression</span>"
    ]
  },
  {
    "objectID": "ch_1_motivation.html#why-linear-regression-fails-for-binary-outcomes",
    "href": "ch_1_motivation.html#why-linear-regression-fails-for-binary-outcomes",
    "title": "1  Introduction to Logistic Regression",
    "section": "1.3 Why linear regression fails for binary outcomes",
    "text": "1.3 Why linear regression fails for binary outcomes\nWith a binary outcome, the relationship between predictors and probability is typically nonlinear: the “same” change in a predictor cannot produce the same change in probability at every starting probability (e.g., you can’t increase a 0.95 probability by +0.2).\nLogistic regression fixes this by modeling a linear relationship on the log-odds (logit) scale, then converting back to the probability scale.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Logistic Regression</span>"
    ]
  },
  {
    "objectID": "ch_1_motivation.html#what-logistic-regression-provides",
    "href": "ch_1_motivation.html#what-logistic-regression-provides",
    "title": "1  Introduction to Logistic Regression",
    "section": "1.4 What logistic regression provides",
    "text": "1.4 What logistic regression provides\nLogistic regression models the probability of an event while ensuring predictions stay between 0 and 1. It also supports effect interpretation via odds ratios and provides likelihood-based tools for inference and model assessment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Logistic Regression</span>"
    ]
  },
  {
    "objectID": "ch_1_motivation.html#footnotes",
    "href": "ch_1_motivation.html#footnotes",
    "title": "1  Introduction to Logistic Regression",
    "section": "",
    "text": "types of random variables are covered in the Introduction to Statistical Inference Short Course, so look there if you need a refresher. Moreover, categorical predictors (factors) were used in the Linear Regression Short Course.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Logistic Regression</span>"
    ]
  },
  {
    "objectID": "ch_2_introduction-to-logistic-regression.html",
    "href": "ch_2_introduction-to-logistic-regression.html",
    "title": "2  Introduction to Logistic Regression",
    "section": "",
    "text": "3 Introduction to Logistic Regression",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Logistic Regression</span>"
    ]
  },
  {
    "objectID": "ch_2_introduction-to-logistic-regression.html#generalised-linear-models",
    "href": "ch_2_introduction-to-logistic-regression.html#generalised-linear-models",
    "title": "2  Introduction to Logistic Regression",
    "section": "3.1 Generalised linear models",
    "text": "3.1 Generalised linear models\nLogistic regression is a special case of a generalised linear model (GLM):\n\nThe outcome distribution is binomial (events out of trials; binary is the 1-trial case)\nThe mean is linked to a linear predictor via the logit link",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Logistic Regression</span>"
    ]
  },
  {
    "objectID": "ch_2_introduction-to-logistic-regression.html#the-logistic-function",
    "href": "ch_2_introduction-to-logistic-regression.html#the-logistic-function",
    "title": "2  Introduction to Logistic Regression",
    "section": "3.2 The logistic function",
    "text": "3.2 The logistic function\nThe logistic function maps any real number to a value between 0 and 1. In R, this is plogis().\n\neta &lt;- c(-4, -2, 0, 2, 4)\ndata.frame(eta = eta, p = plogis(eta))\n\n  eta          p\n1  -4 0.01798621\n2  -2 0.11920292\n3   0 0.50000000\n4   2 0.88079708\n5   4 0.98201379",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Logistic Regression</span>"
    ]
  },
  {
    "objectID": "ch_2_introduction-to-logistic-regression.html#binomial-distribution",
    "href": "ch_2_introduction-to-logistic-regression.html#binomial-distribution",
    "title": "2  Introduction to Logistic Regression",
    "section": "3.3 Binomial distribution",
    "text": "3.3 Binomial distribution\nFor a single observation with probability of success \\(p\\), a binary outcome is:\n\\[\nY \\sim \\text{Bernoulli}(p)\n\\]\nMore generally, for \\(n\\) trials:\n\\[\nY \\sim \\text{Binomial}(n, p)\n\\]\n\ndbinom(x = 0:3, size = 3, prob = 0.4)\n\n[1] 0.216 0.432 0.288 0.064",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Logistic Regression</span>"
    ]
  },
  {
    "objectID": "ch_2_introduction-to-logistic-regression.html#from-probability-to-odds-and-logit",
    "href": "ch_2_introduction-to-logistic-regression.html#from-probability-to-odds-and-logit",
    "title": "2  Introduction to Logistic Regression",
    "section": "3.4 From probability to odds and logit",
    "text": "3.4 From probability to odds and logit\nIf \\(p\\) is the probability of an event, the odds are \\[\n\\\\text{odds} = \\\\frac{p}{1 - p}\n\\] The logit is the log of the odds: \\[\n\\\\text{logit}(p) = \\\\log\\\\left(\\\\frac{p}{1 - p}\\\\right)\n\\]\n\np &lt;- 0.2\nodds &lt;- p / (1 - p)\nlogit &lt;- log(odds)\nc(p = p, odds = odds, logit = logit)\n\n        p      odds     logit \n 0.200000  0.250000 -1.386294 \n\n\n\nA change of 1 unit in the logit scale multiplies the odds by about 2.72 (because \\(e^1 \\\\approx 2.72\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Logistic Regression</span>"
    ]
  },
  {
    "objectID": "ch_2_introduction-to-logistic-regression.html#the-logistic-curve",
    "href": "ch_2_introduction-to-logistic-regression.html#the-logistic-curve",
    "title": "2  Introduction to Logistic Regression",
    "section": "3.5 The logistic curve",
    "text": "3.5 The logistic curve\nThe logit scale is linear, but the probability scale is S-shaped. This is why logistic regression keeps predicted probabilities in the 0 to 1 range.\n\neta &lt;- seq(-6, 6, length.out = 200)\nplot(eta, plogis(eta), type = \"l\", lwd = 2,\n     xlab = \"Linear predictor (eta)\", ylab = \"Probability\")\nabline(h = c(0, 1), col = \"grey80\", lty = 3)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Logistic Regression</span>"
    ]
  },
  {
    "objectID": "ch_3_fitting-logistic-regression-models.html",
    "href": "ch_3_fitting-logistic-regression-models.html",
    "title": "3  Fitting Logistic Regression Models",
    "section": "",
    "text": "4 Fitting Logistic Regression Models",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fitting Logistic Regression Models</span>"
    ]
  },
  {
    "objectID": "ch_3_fitting-logistic-regression-models.html#maximum-likelihood-estimation",
    "href": "ch_3_fitting-logistic-regression-models.html#maximum-likelihood-estimation",
    "title": "3  Fitting Logistic Regression Models",
    "section": "4.1 Maximum likelihood estimation",
    "text": "4.1 Maximum likelihood estimation\nLogistic regression parameters are typically estimated by maximum likelihood. In R, glm(..., family = binomial) fits logistic regression models using likelihood-based methods.\n\nfit_wt &lt;- glm(am ~ wt, data = mtcars, family = binomial)\nlogLik(fit_wt)\n\n'log Lik.' -9.588042 (df=2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fitting Logistic Regression Models</span>"
    ]
  },
  {
    "objectID": "ch_3_fitting-logistic-regression-models.html#fitting-a-model-in-r",
    "href": "ch_3_fitting-logistic-regression-models.html#fitting-a-model-in-r",
    "title": "3  Fitting Logistic Regression Models",
    "section": "4.2 Fitting a model in R",
    "text": "4.2 Fitting a model in R\n\nfit_wt &lt;- glm(am ~ wt, data = mtcars, family = binomial)\nsummary(fit_wt)\n\n\nCall:\nglm(formula = am ~ wt, family = binomial, data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   12.040      4.510   2.670  0.00759 **\nwt            -4.024      1.436  -2.801  0.00509 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 19.176  on 30  degrees of freedom\nAIC: 23.176\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nKey-point 3.1\nCoefficients are on the log-odds scale. To interpret them as odds ratios, exponentiate with exp().",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fitting Logistic Regression Models</span>"
    ]
  },
  {
    "objectID": "ch_3_fitting-logistic-regression-models.html#interpreting-coefficients",
    "href": "ch_3_fitting-logistic-regression-models.html#interpreting-coefficients",
    "title": "3  Fitting Logistic Regression Models",
    "section": "4.3 Interpreting coefficients",
    "text": "4.3 Interpreting coefficients\n\nfit_ex &lt;- glm(am ~ wt + hp, data = mtcars, family = binomial)\ncoef(fit_ex)\n\n(Intercept)          wt          hp \n 18.8662987  -8.0834752   0.0362556 \n\nexp(coef(fit_ex))\n\n (Intercept)           wt           hp \n1.561455e+08 3.085967e-04 1.036921e+00 \n\n\n\nExercise 3.1\nCompute the odds ratio for wt and store it as or_wt.\n\n\n\n\n\n\n\n\n\n\n\nor_wt &lt;- exp(coef(fit_ex)[\"wt\"])\nor_wt\nor_wt &lt;- exp(coef(fit_ex)[\"wt\"])\nor_wt",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fitting Logistic Regression Models</span>"
    ]
  },
  {
    "objectID": "ch_3_fitting-logistic-regression-models.html#inference-in-logistic-regression",
    "href": "ch_3_fitting-logistic-regression-models.html#inference-in-logistic-regression",
    "title": "3  Fitting Logistic Regression Models",
    "section": "4.4 Inference in logistic regression",
    "text": "4.4 Inference in logistic regression\nFor quick inference, you can use Wald tests from summary() and Wald confidence intervals via confint.default().\n\nsummary(fit_ex)\n\n\nCall:\nglm(formula = am ~ wt + hp, family = binomial, data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) 18.86630    7.44356   2.535  0.01126 * \nwt          -8.08348    3.06868  -2.634  0.00843 **\nhp           0.03626    0.01773   2.044  0.04091 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 10.059  on 29  degrees of freedom\nAIC: 16.059\n\nNumber of Fisher Scoring iterations: 8\n\nconfint.default(fit_ex)\n\n                    2.5 %     97.5 %\n(Intercept)   4.277193002 33.4554044\nwt          -14.097967884 -2.0689825\nhp            0.001497294  0.0710139",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fitting Logistic Regression Models</span>"
    ]
  },
  {
    "objectID": "ch_3_fitting-logistic-regression-models.html#categorical-predictors",
    "href": "ch_3_fitting-logistic-regression-models.html#categorical-predictors",
    "title": "3  Fitting Logistic Regression Models",
    "section": "4.5 Categorical predictors",
    "text": "4.5 Categorical predictors\n\nfit_cat &lt;- glm(am ~ wt + factor(cyl), data = mtcars, family = binomial)\nsummary(fit_cat)\n\n\nCall:\nglm(formula = am ~ wt + factor(cyl), family = binomial, data = mtcars)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)    20.853      8.032   2.596  0.00942 **\nwt             -7.859      3.055  -2.573  0.01009 * \nfactor(cyl)6    3.105      2.425   1.280  0.20042   \nfactor(cyl)8    5.379      3.201   1.681  0.09281 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 14.661  on 28  degrees of freedom\nAIC: 22.661\n\nNumber of Fisher Scoring iterations: 7\n\n\nWhen a predictor is a factor, coefficients compare each level to a reference level. Interpret them as odds ratios relative to that baseline.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fitting Logistic Regression Models</span>"
    ]
  },
  {
    "objectID": "ch_4_assessing-model-fit.html",
    "href": "ch_4_assessing-model-fit.html",
    "title": "4  Assessing Model Fit",
    "section": "",
    "text": "5 Assessing Model Fit",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assessing Model Fit</span>"
    ]
  },
  {
    "objectID": "ch_4_assessing-model-fit.html#deviance",
    "href": "ch_4_assessing-model-fit.html#deviance",
    "title": "4  Assessing Model Fit",
    "section": "5.1 Deviance",
    "text": "5.1 Deviance\nDeviance compares a fitted model to a saturated model (perfect fit). Smaller deviance generally indicates better fit, and differences in deviance can be used for likelihood ratio tests.\n\nfit_null &lt;- glm(am ~ 1, data = mtcars, family = binomial)\nfit_full &lt;- glm(am ~ wt + hp + factor(cyl), data = mtcars, family = binomial)\n\nanova(fit_null, fit_full, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: am ~ 1\nModel 2: am ~ wt + hp + factor(cyl)\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1        31     43.230                          \n2        27      7.255  4   35.974 2.929e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nKey-point 4.1\nLogistic regression does not use R-squared in the same way as linear regression. Model comparison is typically done with likelihood-based tools (deviance, likelihood ratio tests) and information criteria (AIC).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assessing Model Fit</span>"
    ]
  },
  {
    "objectID": "ch_4_assessing-model-fit.html#goodness-of-fit-tests",
    "href": "ch_4_assessing-model-fit.html#goodness-of-fit-tests",
    "title": "4  Assessing Model Fit",
    "section": "5.2 Goodness-of-fit tests",
    "text": "5.2 Goodness-of-fit tests\nSome goodness-of-fit tests (e.g., Hosmer–Lemeshow) require additional packages and choices about grouping. In practice, you should combine multiple checks:\n\nLikelihood-based comparisons between nested models\nPredictive performance (next chapter)\nResidual and influence diagnostics for unusual points",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assessing Model Fit</span>"
    ]
  },
  {
    "objectID": "ch_4_assessing-model-fit.html#pseudo-r-squared",
    "href": "ch_4_assessing-model-fit.html#pseudo-r-squared",
    "title": "4  Assessing Model Fit",
    "section": "5.3 Pseudo R-squared",
    "text": "5.3 Pseudo R-squared\nPseudo R-squared measures (e.g., McFadden’s) are based on likelihoods and are best used for rough comparisons rather than as a direct analogue of linear-model R-squared.\n\nfit_null &lt;- glm(am ~ 1, data = mtcars, family = binomial)\nfit_full &lt;- glm(am ~ wt + hp + factor(cyl), data = mtcars, family = binomial)\n\nmcfadden_r2 &lt;- 1 - as.numeric(logLik(fit_full) / logLik(fit_null))\nmcfadden_r2\n\n[1] 0.8321671",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assessing Model Fit</span>"
    ]
  },
  {
    "objectID": "ch_4_assessing-model-fit.html#aic",
    "href": "ch_4_assessing-model-fit.html#aic",
    "title": "4  Assessing Model Fit",
    "section": "5.4 AIC",
    "text": "5.4 AIC\n\nAIC(fit_null, fit_full)\n\n         df      AIC\nfit_null  1 45.22973\nfit_full  5 17.25537",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assessing Model Fit</span>"
    ]
  },
  {
    "objectID": "ch_5_predictions-from-logistic-regression-models.html",
    "href": "ch_5_predictions-from-logistic-regression-models.html",
    "title": "5  Predictions from Logistic Regression Models",
    "section": "",
    "text": "6 Predictions from Logistic Regression Models",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Predictions from Logistic Regression Models</span>"
    ]
  },
  {
    "objectID": "ch_5_predictions-from-logistic-regression-models.html#generating-predicted-probabilities",
    "href": "ch_5_predictions-from-logistic-regression-models.html#generating-predicted-probabilities",
    "title": "5  Predictions from Logistic Regression Models",
    "section": "6.1 Generating predicted probabilities",
    "text": "6.1 Generating predicted probabilities\n\nfit &lt;- glm(am ~ wt + hp, data = mtcars, family = binomial)\nnew_cars &lt;- data.frame(wt = c(2.2, 3.2, 3.8), hp = c(110, 110, 110))\npredict(fit, newdata = new_cars, type = \"response\")\n\n           1            2            3 \n0.9937615708 0.0468551348 0.0003846502 \n\n\nYou can also visualise how predicted probability changes with a predictor (holding other predictors constant).\n\nfit &lt;- glm(am ~ wt + hp, data = mtcars, family = binomial)\ngrid &lt;- data.frame(\n  wt = seq(min(mtcars$wt), max(mtcars$wt), length.out = 50),\n  hp = mean(mtcars$hp)\n)\ngrid$prob &lt;- predict(fit, newdata = grid, type = \"response\")\nplot(grid$wt, grid$prob, type = \"l\", lwd = 2,\n     xlab = \"Weight\", ylab = \"Predicted probability\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Predictions from Logistic Regression Models</span>"
    ]
  },
  {
    "objectID": "ch_5_predictions-from-logistic-regression-models.html#classification-thresholds",
    "href": "ch_5_predictions-from-logistic-regression-models.html#classification-thresholds",
    "title": "5  Predictions from Logistic Regression Models",
    "section": "6.2 Classification thresholds",
    "text": "6.2 Classification thresholds\nIf you turn probabilities into classes (0/1), you need a threshold. Changing the threshold trades sensitivity against specificity.\n\nthreshold &lt;- 0.5\nclass_hat &lt;- factor(ifelse(prob &gt;= threshold, 1, 0), levels = c(0, 1))\nactual &lt;- factor(mtcars$am, levels = c(0, 1))\nconf_mat &lt;- table(Predicted = class_hat, Actual = actual)\nconf_mat\n\n         Actual\nPredicted  0  1\n        0 18  1\n        1  1 12\n\n\n\ntrue_pos &lt;- conf_mat[\"1\", \"1\"]\ntrue_neg &lt;- conf_mat[\"0\", \"0\"]\nfalse_pos &lt;- conf_mat[\"1\", \"0\"]\nfalse_neg &lt;- conf_mat[\"0\", \"1\"]\n\naccuracy &lt;- (true_pos + true_neg) / sum(conf_mat)\nsensitivity &lt;- true_pos / (true_pos + false_neg)\nspecificity &lt;- true_neg / (true_neg + false_pos)\n\nc(accuracy = accuracy, sensitivity = sensitivity, specificity = specificity)\n\n   accuracy sensitivity specificity \n  0.9375000   0.9230769   0.9473684",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Predictions from Logistic Regression Models</span>"
    ]
  },
  {
    "objectID": "ch_5_predictions-from-logistic-regression-models.html#roc-curves-and-auc",
    "href": "ch_5_predictions-from-logistic-regression-models.html#roc-curves-and-auc",
    "title": "5  Predictions from Logistic Regression Models",
    "section": "6.3 ROC curves and AUC",
    "text": "6.3 ROC curves and AUC\n\nroc_thresholds &lt;- seq(1, 0, by = -0.02)\nactual &lt;- factor(mtcars$am, levels = c(0, 1))\nactual_num &lt;- as.integer(actual) - 1\n\ntpr &lt;- sapply(roc_thresholds, function(t) {\n  pred &lt;- ifelse(prob &gt;= t, 1, 0)\n  tp &lt;- sum(pred == 1 & actual_num == 1)\n  fn &lt;- sum(pred == 0 & actual_num == 1)\n  tp / (tp + fn)\n})\n\nfpr &lt;- sapply(roc_thresholds, function(t) {\n  pred &lt;- ifelse(prob &gt;= t, 1, 0)\n  fp &lt;- sum(pred == 1 & actual_num == 0)\n  tn &lt;- sum(pred == 0 & actual_num == 0)\n  fp / (fp + tn)\n})\n\nroc_df &lt;- data.frame(fpr = fpr, tpr = tpr)\nroc_df &lt;- roc_df[order(roc_df$fpr), ]\n\nplot(roc_df$fpr, roc_df$tpr, type = \"l\", lwd = 2,\n     xlab = \"False positive rate\", ylab = \"True positive rate\")\nabline(0, 1, lty = 3, col = \"grey60\")\n\n\n\n\n\n\n\nauc &lt;- sum(diff(roc_df$fpr) * (head(roc_df$tpr, -1) + tail(roc_df$tpr, -1)) / 2)\nauc\n\n[1] 0.9878543\n\n\n\nKey-point 5.1\nAUC near 0.5 indicates random guessing, while values closer to 1 indicate stronger discrimination.\n\n\n\nExercise 5.1\nUsing a threshold of 0.6, compute the accuracy and store it as accuracy_06.\n\n\n\n\n\n\n\n\n\n\n\nthreshold &lt;- 0.6\nclass_hat &lt;- ifelse(prob &gt;= threshold, 1, 0)\naccuracy_06 &lt;- mean(class_hat == actual_num)\naccuracy_06\nthreshold &lt;- 0.6\nclass_hat &lt;- ifelse(prob &gt;= threshold, 1, 0)\naccuracy_06 &lt;- mean(class_hat == actual_num)\naccuracy_06",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Predictions from Logistic Regression Models</span>"
    ]
  },
  {
    "objectID": "ch_6_extensions-case-study.html",
    "href": "ch_6_extensions-case-study.html",
    "title": "6  Module 4: Extensions and a Mini Case Study",
    "section": "",
    "text": "7 Extensions and a Mini Case Study",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Module 4: Extensions and a Mini Case Study</span>"
    ]
  },
  {
    "objectID": "ch_6_extensions-case-study.html#learning-outcomes",
    "href": "ch_6_extensions-case-study.html#learning-outcomes",
    "title": "6  Module 4: Extensions and a Mini Case Study",
    "section": "7.1 Learning outcomes",
    "text": "7.1 Learning outcomes\n\nFit multivariable logistic models with categorical predictors.\nUse interactions when effects differ by group.\nSummarize results as predicted probabilities for realistic scenarios.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Module 4: Extensions and a Mini Case Study</span>"
    ]
  },
  {
    "objectID": "ch_6_extensions-case-study.html#multivariable-logistic-regression",
    "href": "ch_6_extensions-case-study.html#multivariable-logistic-regression",
    "title": "6  Module 4: Extensions and a Mini Case Study",
    "section": "7.2 Multivariable logistic regression",
    "text": "7.2 Multivariable logistic regression\n\nfit_multi &lt;- glm(am ~ wt + hp + factor(cyl), data = mtcars, family = binomial)\nsummary(fit_multi)\n\n\nCall:\nglm(formula = am ~ wt + hp + factor(cyl), family = binomial, \n    data = mtcars)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)   18.09383    9.46271   1.912   0.0559 .\nwt           -10.67598    5.44182  -1.962   0.0498 *\nhp             0.10321    0.09606   1.074   0.2826  \nfactor(cyl)6   2.76575    3.15679   0.876   0.3810  \nfactor(cyl)8  -8.38896   13.16709  -0.637   0.5240  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  7.2554  on 27  degrees of freedom\nAIC: 17.255\n\nNumber of Fisher Scoring iterations: 9\n\n\n\nKey-point 6.1\nEach coefficient is a partial effect on the log-odds scale, holding the other predictors constant.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Module 4: Extensions and a Mini Case Study</span>"
    ]
  },
  {
    "objectID": "ch_6_extensions-case-study.html#interactions-for-subgroup-effects",
    "href": "ch_6_extensions-case-study.html#interactions-for-subgroup-effects",
    "title": "6  Module 4: Extensions and a Mini Case Study",
    "section": "7.3 Interactions for subgroup effects",
    "text": "7.3 Interactions for subgroup effects\nIf the effect of a predictor changes across groups, use an interaction term.\n\nfit_interaction &lt;- glm(am ~ wt * factor(cyl), data = mtcars, family = binomial)\nsummary(fit_interaction)\n\n\nCall:\nglm(formula = am ~ wt * factor(cyl), family = binomial, data = mtcars)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)        13.758      7.787   1.767   0.0773 .\nwt                 -5.116      2.984  -1.714   0.0865 .\nfactor(cyl)6      313.415  71542.593   0.004   0.9965  \nfactor(cyl)8       14.544     22.810   0.638   0.5237  \nwt:factor(cyl)6  -102.300  23338.031  -0.004   0.9965  \nwt:factor(cyl)8    -3.340      6.854  -0.487   0.6261  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 12.593  on 26  degrees of freedom\nAIC: 24.593\n\nNumber of Fisher Scoring iterations: 19",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Module 4: Extensions and a Mini Case Study</span>"
    ]
  },
  {
    "objectID": "ch_6_extensions-case-study.html#communicating-results-with-scenarios",
    "href": "ch_6_extensions-case-study.html#communicating-results-with-scenarios",
    "title": "6  Module 4: Extensions and a Mini Case Study",
    "section": "7.4 Communicating results with scenarios",
    "text": "7.4 Communicating results with scenarios\n\nnew_cars &lt;- data.frame(\n  wt = c(2.4, 3.2),\n  hp = c(100, 150),\n  cyl = factor(c(4, 6))\n)\n\npredict(fit_multi, newdata = new_cars, type = \"response\")\n\n        1         2 \n0.9422625 0.8982382",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Module 4: Extensions and a Mini Case Study</span>"
    ]
  },
  {
    "objectID": "ch_6_extensions-case-study.html#mini-case-study-template-dataset-to-be-added",
    "href": "ch_6_extensions-case-study.html#mini-case-study-template-dataset-to-be-added",
    "title": "6  Module 4: Extensions and a Mini Case Study",
    "section": "7.5 Mini case study template (dataset to be added)",
    "text": "7.5 Mini case study template (dataset to be added)\nUse this checklist to structure the final case study once the dataset is chosen.\n\nDefine the outcome variable and clarify what counts as an event.\nSummarize the baseline event rate and missing data.\nChoose a small set of predictors with clear interpretations.\nFit a logistic model and report odds ratios and predicted probabilities.\nEvaluate the model with an appropriate threshold and a confusion matrix.\nCommunicate results in plain language with scenario predictions.\n\n\nNote 6.1\nThe final case study will be updated once the outcome variable is confirmed for the selected research dataset.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Module 4: Extensions and a Mini Case Study</span>"
    ]
  },
  {
    "objectID": "ch_6_extensions-case-study.html#reporting-template",
    "href": "ch_6_extensions-case-study.html#reporting-template",
    "title": "6  Module 4: Extensions and a Mini Case Study",
    "section": "7.6 Reporting template",
    "text": "7.6 Reporting template\nUse this outline to write up results in plain language.\n\nOutcome definition, sample size, and event rate.\nModel specification (predictors and reference levels).\nKey effects as odds ratios with a short interpretation.\nPredicted probabilities for 2 to 3 realistic scenarios.\nPerformance summary (threshold, confusion matrix metrics, AUC).\nLimitations and next steps.\n\nTemplate paragraph:\n“We modeled [outcome] using logistic regression with predictors [list]. The event rate was [rate]. A one-unit increase in [predictor] was associated with [odds ratio] times the odds of the event, holding other variables constant. For a typical case ([scenario]), the predicted probability was [probability]. Using a threshold of [threshold], the model achieved [accuracy/sensitivity/specificity] and an AUC of [AUC]. Key limitations include [limitations].”",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Module 4: Extensions and a Mini Case Study</span>"
    ]
  },
  {
    "objectID": "ch_6_extensions-case-study.html#practice-prompts",
    "href": "ch_6_extensions-case-study.html#practice-prompts",
    "title": "6  Module 4: Extensions and a Mini Case Study",
    "section": "7.7 Practice prompts",
    "text": "7.7 Practice prompts\n\nFit the model with factor(gear) instead of factor(cyl). What changes?\nCompare predictions from fit_multi and fit_interaction for the same scenario.\nDraft a one-paragraph summary for a non-technical audience.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Module 4: Extensions and a Mini Case Study</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Logistic regression\nA regression model for binary outcomes that models the log-odds of an event as a linear function of predictors.\n\n\nBinary outcome\nA response variable with two possible values, often coded as 0 and 1.\n\n\nProbability\nA number between 0 and 1 that represents the chance of an event.\n\n\nOdds\nThe ratio of the probability of an event to the probability of it not occurring: p / (1 - p).\n\n\nLogit\nThe log of the odds: log(p / (1 - p)).\n\n\nLink function\nA function that connects the mean of the response to the linear predictor; for logistic regression the link is the logit.\n\n\nLinear predictor\nThe linear combination of predictors, for example beta0 + beta1 x.\n\n\nCoefficient\nA model parameter that describes how a predictor changes the log-odds, holding other predictors constant.\n\n\nOdds ratio\nThe multiplicative change in odds for a one-unit increase in a predictor; exp(coefficient).\n\n\nPredicted probability\nThe model’s estimate of the probability of an event for a given set of predictors.\n\n\nLikelihood\nA measure of how well a model explains the observed data; higher is better.\n\n\nDeviance\nA goodness-of-fit measure based on the likelihood; lower is better.\n\n\nAkaike Information Criterion\nA model comparison metric that balances fit and complexity; lower values indicate a preferred model among those compared.\n\n\nConfusion matrix\nA table of predicted versus actual outcomes used to summarize classification performance.\n\n\nSensitivity\nThe proportion of true positives correctly identified by the model.\n\n\nSpecificity\nThe proportion of true negatives correctly identified by the model.\n\n\nAccuracy\nThe proportion of all predictions that are correct.\n\n\nROC curve\nA curve showing the trade-off between sensitivity and 1 - specificity across thresholds.\n\n\nAUC\nArea under the ROC curve; a summary of classification performance across thresholds.\n\n\nCalibration\nHow closely predicted probabilities match observed event rates.\n\n\nSeparation\nA situation where predictors perfectly separate outcomes, leading to unstable estimates.\n\n\nClass imbalance\nWhen one outcome class is much more frequent than the other.\n\n\nInteraction\nA term that allows the effect of one predictor to depend on another.\n\n\nFactor\nA categorical predictor in R that is represented by discrete levels.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "ch_1_motivation.html#sec-glm",
    "href": "ch_1_motivation.html#sec-glm",
    "title": "1  Introduction to Logistic Regression",
    "section": "1.2 Generalised Linear Models",
    "text": "1.2 Generalised Linear Models",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Logistic Regression</span>"
    ]
  }
]